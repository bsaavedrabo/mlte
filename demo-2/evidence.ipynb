{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect Evidence\n",
    "\n",
    "In the second phase of SDMT, we collect _evidence_ to attest to the fact that the model realized the properties specified in the previous phase.\n",
    "\n",
    "We define and instantiate `Measurement`s to generate this evidence. Each individual piece of evidence is a `Value`. Once `Value`s are produced, we can persist them to an _artifact store_ to maintain our evidence across sessions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries for loading the package locally\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def package_root() -> str:\n",
    "    \"\"\"Resolve the path to the project root.\"\"\"\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src/\"))\n",
    "\n",
    "sys.path.append(package_root())\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlte\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(store_path, exist_ok=True)\n",
    "\n",
    "mlte.set_model(\"OxfordFlower\", \"0.0.1\")\n",
    "mlte.set_artifact_store_uri(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairnesss Measurements\n",
    "\n",
    "Evidence collected in this section checks for fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "\n",
    "import garden\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which datasets are stored\n",
    "DATASETS_DIR = Path(os.getcwd()) / \"data\"\n",
    "\n",
    "\n",
    "def load_data(data_folder: str):\n",
    "    \"\"\"Loads all garden data results and taxonomy categories.\"\"\"\n",
    "    df_results = garden.load_base_results(data_folder)\n",
    "    df_results.head()\n",
    "\n",
    "    # Load the taxonomic data and merge with results.\n",
    "    df_info = garden.load_taxonomy(data_folder)\n",
    "    df_results.rename(columns = {'label':'Label'}, inplace = True)\n",
    "    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n",
    "\n",
    "    return df_info, df_all\n",
    "\n",
    "\n",
    "def split_data(df_info, df_all):\n",
    "    \"\"\"Splits the data into 3 different populations to evaluate them.\"\"\"\n",
    "    df_gardenpop = df_info.copy()\n",
    "    df_gardenpop['Population1'] = (np.around(np.random.dirichlet\n",
    "                            (np.ones(df_gardenpop.shape[0]),size=1)[0],\n",
    "                            decimals = 3) *1000).astype(int)\n",
    "    df_gardenpop['Population2'] = (np.around(np.random.dirichlet\n",
    "                            (np.ones(df_gardenpop.shape[0]),size=1)[0],\n",
    "                            decimals = 3) *1000).astype(int)\n",
    "    df_gardenpop['Population3'] = (np.around(np.random.dirichlet\n",
    "                            (np.ones(df_gardenpop.shape[0]),size=1)[0],\n",
    "                            decimals = 3) *1000).astype(int)\n",
    "    df_gardenpop\n",
    "\n",
    "    #build populations from test data set that match the garden compositions\n",
    "    from random import choices\n",
    "\n",
    "    #build 3 gardens with populations of 1000.\n",
    "    pop_names = ['Population1', 'Population2', 'Population3']\n",
    "    gardenpops = np.zeros( (3,1000), int)\n",
    "    gardenmems = np.zeros( (3,1000), int)\n",
    "\n",
    "    for j in range(1000):\n",
    "        for i in range(len(df_gardenpop)):\n",
    "            my_flower = df_gardenpop.iloc[i]['Common Name']\n",
    "        \n",
    "            for g in range(3):\n",
    "                n_choices = df_gardenpop.iloc[i][pop_names[g]]\n",
    "                my_choices = df_all[df_all['Common Name'] == my_flower]['model correct'].to_list()\n",
    "                my_selection = choices(my_choices, k=n_choices)\n",
    "            \n",
    "                gardenpops[g][j] += sum(my_selection)\n",
    "                gardenmems[g][j] += len(my_selection)\n",
    "\n",
    "    gardenpops\n",
    "\n",
    "    return gardenpops, gardenmems\n",
    "\n",
    "\n",
    "def calculate_model_performance_acc(gardenpops, gardenmems):\n",
    "    \"\"\"Get accucray of models across the garden populations\"\"\"\n",
    "    gardenacc = np.zeros( (3,1000), float)\n",
    "    for i in range (1000):\n",
    "        for g in range(3):\n",
    "            gardenacc[g][i] = gardenpops[g][i]/gardenmems[g][i]\n",
    "    gardenacc\n",
    "\n",
    "    model_performance_acc = []\n",
    "    for g in range(3):\n",
    "        avg = round(np.average(gardenacc[g][:]),3)\n",
    "        std = round(np.std(gardenacc[g][:]),3)\n",
    "        min = round(np.amin(gardenacc[g][:]),3)\n",
    "        max = round(np.amax(gardenacc[g][:]),3)\n",
    "        model_performance_acc.append(round(avg,3))\n",
    "        \n",
    "        print(\"%1d %1.3f %1.3f %1.3f %1.3f\" % (g, avg, std, min, max))\n",
    "\n",
    "    return model_performance_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data.\n",
    "data = load_data(DATASETS_DIR)\n",
    "split_data = split_data(data[0], data[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, we simply wrap the output from `accuracy_score` with a custom `Result` type to cope with the output of a third-party library that is not supported by a MLTE builtin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_accuracy import MultipleAccuracy\n",
    "from mlte.measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the Spec.\n",
    "accuracy_measurement = ExternalMeasurement(\"accuracy across gardens\", MultipleAccuracy, calculate_model_performance_acc)\n",
    "accuracy = accuracy_measurement.evaluate(split_data[0], split_data[1])\n",
    "\n",
    "# Inspect value\n",
    "print(accuracy)\n",
    "\n",
    "# Save to artifact store\n",
    "accuracy.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robustness Measurements\n",
    "\n",
    "Evidence collected in this section checks for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    #Calculate the base model accuracy result per data label\n",
    "    df_pos = df_results[df_results['model correct'] == True].groupby('label').count()\n",
    "    df_pos.drop(columns = ['prediced_label'], inplace = True)\n",
    "    df_neg = df_results[df_results['model correct'] == False].groupby('label').count()\n",
    "    df_neg.drop(columns = ['prediced_label'], inplace = True)\n",
    "    df_neg.rename(columns = {'model correct':'model incorrect'}, inplace = True)\n",
    "    df_res = df_pos.merge(df_neg, right_on ='label', left_on = 'label', how = 'outer')\n",
    "    df_res.fillna(0, inplace = True)\n",
    "    df_res['model acc'] = df_res['model correct'] / ( df_res['model correct'] + df_res['model incorrect'] )\n",
    "    df_res['count'] = ( df_res['model correct'] + df_res['model incorrect'] )\n",
    "    df_res.drop(columns = ['model correct', 'model incorrect'], inplace = True)\n",
    "    df_res.head()\n",
    "\n",
    "    return df_res\n",
    "\n",
    "def calculate_accuracy_per_set(data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame) -> pd.DataFrame:\n",
    "    #Calculate the model accuracy per data label for each blurred data set\n",
    "    base_filename = 'FlowerModelv1_TestSetResults'\n",
    "    ext_filename = '.csv'\n",
    "    set_filename = ['_blur2x8', '_blur5x8', '_blur0x8', '_noR', '_noG','_noB']\n",
    "\n",
    "    col_root = 'model acc'\n",
    "\n",
    "    for fs in set_filename:\n",
    "        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n",
    "        colname = col_root + fs\n",
    "        \n",
    "        df_temp = pd.read_csv(filename)\n",
    "        df_temp.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "        \n",
    "        df_pos = df_temp[df_temp['model correct'] == True].groupby('label').count()\n",
    "        df_pos.drop(columns = ['prediced_label'], inplace = True)\n",
    "        df_neg = df_results[df_results['model correct'] == False].groupby('label').count()\n",
    "        df_neg.drop(columns = ['prediced_label'], inplace = True)\n",
    "        df_neg.rename(columns = {'model correct':'model incorrect'}, inplace = True)\n",
    "        df_res2 = df_pos.merge(df_neg, right_on ='label', left_on = 'label', how = 'outer')\n",
    "        df_res2.fillna(0, inplace = True)\n",
    "        \n",
    "        df_res2[colname] = df_res2['model correct'] / ( df_res2['model correct'] + df_res2['model incorrect'] )\n",
    "        df_res2.drop(columns = ['model correct', 'model incorrect'], inplace = True)\n",
    "        \n",
    "        df_res = df_res.merge(df_res2, right_on = 'label', left_on = 'label', how = 'outer')\n",
    "\n",
    "    df_res.head()\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all data.\n",
    "df_results = garden.load_base_results(DATASETS_DIR)\n",
    "df_res = calculate_base_accuracy(df_results)\n",
    "df_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\n",
    "df_info = garden.load_taxonomy(DATASETS_DIR)\n",
    "df_all = garden.merge_taxonomy_with_results(df_res, df_info, \"label\", \"Label\")\n",
    "\n",
    "#fill in missing model accuracy data\n",
    "df_all['model acc_noR'].fillna(0, inplace = True)\n",
    "df_all['model acc_noG'].fillna(0, inplace = True)\n",
    "df_all['model acc_noB'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the actual measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view changes in model accuracy\n",
    "model_acc = sum( df_res['model acc'] * df_res['count'] ) / sum( df_res['count'] )\n",
    "print('base model accuracy' , model_acc)\n",
    "model_acc = sum( df_res['model acc_blur2x8'] * df_res['count'] ) / sum( df_res['count'] )\n",
    "print('model accuracy with 2x8 blur' , model_acc)\n",
    "model_acc = sum( df_res['model acc_blur5x8'] * df_res['count'] ) / sum( df_res['count'] )\n",
    "print('model accuracy with 5x8 blur' , model_acc)\n",
    "model_acc = sum( df_res['model acc_blur0x8'] * df_res['count'] ) / sum( df_res['count'] )\n",
    "print('model accuracy with 0x8 blur' , model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "print( scipy.stats.ranksums( df_res['model acc'] , df_res['model acc_blur2x8']) )\n",
    "print( scipy.stats.ranksums( df_res['model acc'] , df_res['model acc_blur5x8']) )\n",
    "print( scipy.stats.ranksums( df_res['model acc'] , df_res['model acc_blur0x8']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "my_results = [scipy.stats.ranksums( df_res['model acc'] , df_res['model acc_blur2x8']),\n",
    "              scipy.stats.ranksums( df_res['model acc'] , df_res['model acc_blur5x8']),\n",
    "              scipy.stats.ranksums( df_res['model acc'] , df_res['model acc_blur0x8'])]\n",
    "print(my_results)\n",
    "\n",
    "my_blur = ['2x8', '5x8', '0x8']\n",
    "for i in range(len(my_blur)):\n",
    "    blur = my_blur[i]\n",
    "    stat, pval = my_results[i]\n",
    "    if (pval < 0.05/len(my_blur)):\n",
    "        print('there is a significant difference between origional model with no blur and blur ', blur)\n",
    "        print('\\twith p-value ', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we see a drop in model accuracy with increasing blur. But aside from max blur (0x8), the model accuracy fall off isn't bad.  \n",
    "Now to next part of the question- is this equal across the phylogenic groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the initial result, blur columns to anaylze effect of blur\n",
    "df_all['delta_2x8'] = df_all['model acc'] - df_all['model acc_blur2x8']\n",
    "df_all['delta_5x8'] = df_all['model acc'] - df_all['model acc_blur5x8']\n",
    "df_all['delta_0x8'] = df_all['model acc'] - df_all['model acc_blur0x8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check Clade2\n",
    "pops = df_all['Clade2'].unique().tolist()\n",
    "blurs = ['delta_2x8', 'delta_5x8', 'delta_0x8',]\n",
    "\n",
    "my_results= []\n",
    "\n",
    "for blr in blurs:\n",
    "    print('\\n', blr)\n",
    "    for pop1 in pops:\n",
    "        for pop2 in pops:\n",
    "            \n",
    "            #print( pop1, pop2, scipy.stats.ranksums( df_all[df_all['Clade2'] == pop1 ][blr] ,\n",
    "            #                            df_all[df_all['Clade2'] == pop2 ][blr] ) )\n",
    "            \n",
    "            stat, pval = scipy.stats.ranksums( df_all[df_all['Clade2'] == pop1 ][blr] ,\n",
    "                                               df_all[df_all['Clade2'] == pop2 ][blr] )\n",
    "            \n",
    "            if pval < 0.05/len(pops):\n",
    "                statement = 'Significant difference between'+ pop1+ pop2+ 'with'+ scipy.stats.ranksums( df_all[df_all['Clade2'] == pop1 ][blr] ,\n",
    "                                        df_all[df_all['Clade2'] == pop2 ][blr] ) \n",
    "                my_results.push(statement)\n",
    "                print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(my_results) ==0):\n",
    "    print('There is not a significant difference at Clade 2 level in model performance for any blur level')\n",
    "else:\n",
    "    print('There is a significant difference at Clade 2 level in model performance for any blur level, between')\n",
    "    print(my_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_now = df_all[['Clade2', 'Clade 3']].copy().groupby(['Clade2', 'Clade 3']).count().reset_index()\n",
    "ps1 = df_now['Clade2'].to_list()\n",
    "ps2 = df_now['Clade 3'].to_list()\n",
    "print(df_now)\n",
    "\n",
    "my_record = []\n",
    "\n",
    "for blr in blurs:\n",
    "    print('\\n', blr) \n",
    "    for i in range(len(ps1)):\n",
    "        p1c1 = ps1[i]\n",
    "        p1c2 = ps2[i]\n",
    "        for j in range(len(ps1)):\n",
    "            p2c1 = ps1[j]\n",
    "            p2c2 = ps2[j]\n",
    "            if (len(df_all[(df_all['Clade2'] == p1c1) & (df_all['Clade 3'] == p2c2)][blr])>0 | \n",
    "                len(df_all[(df_all['Clade2'] == p2c1) & (df_all['Clade 3'] == p2c2)][blr])>0):\n",
    "                \n",
    "                stat, pval = scipy.stats.ranksums( df_all[(df_all['Clade2'] == p1c1) & (df_all['Clade 3'] == p2c2)][blr] ,\n",
    "                                        df_all[(df_all['Clade2'] == p2c1) & (df_all['Clade 3'] == p2c2)][blr] )\n",
    "\n",
    "                if (pval < (0.05/len(ps1))):\n",
    "                    print('(', p1c1, p2c2,'), (', p2c1, p2c2 ,')', 'RanksumsResult(statistic=', stat,', ','pvalue=', pval,')')\n",
    "                    statement = '(' + p1c1, p2c2 + '), (' + p2c1, p2c2 +');'\n",
    "                    my_record.append(statement)\n",
    "            #print('(', p1c1, p2c2,'), (', p2c1, p2c2 ,')',  \n",
    "            #      scipy.stats.ranksums( df_all[(df_all['Clade2'] == p1c1) & (df_all['Clade 3'] == p2c2)][blr] ,\n",
    "            #                            df_all[(df_all['Clade2'] == p2c1) & (df_all['Clade 3'] == p2c2)][blr] ) )\n",
    "            #print( 'here', len(df_all[(df_all['Clade2'] == p1c1) & (df_all['Clade 3'] == p1c2)][blr]) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(my_record) == 0):\n",
    "    #no pairs with signifigant difference were found, so the test would pass\n",
    "    print('No signficant difference among clades was found')\n",
    "else:\n",
    "    print('A signficant difference among clades was found amoung the following clade paris:')\n",
    "    print(my_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
