{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation and Report Generation\n",
    "\n",
    "The final phase of SDMT involves aggregating evidence, validating the metrics reflected by the evidence we collected, and displaying this information in a report."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(store_path, exist_ok=True)   # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"ns\", \"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which reports are stored\n",
    "REPORTS_DIR = Path(os.getcwd()) / \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Values and get an updated `ValidatedSpec` with `Result`s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `Spec` ready and we have enough evidence, we create a `SpecValidator` with our spec, and add all the `Value`s we have. With that we can validate our spec and generate an output `ValidatedSpec`, with the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "from mlte.validation.spec_validator import SpecValidator\n",
    "\n",
    "from mlte.measurement.cpu import CPUStatistics\n",
    "from mlte.measurement.memory import MemoryStatistics\n",
    "from mlte.value.types.image import Image\n",
    "from mlte.value.types.integer import Integer\n",
    "\n",
    "from values.multiple_accuracy import MultipleAccuracy\n",
    "from values.multiple_ranksums import MultipleRanksums\n",
    "from values.ranksums import RankSums\n",
    "\n",
    "# Load the specification\n",
    "spec = Spec.load()\n",
    "\n",
    "# Add all values to the validator.\n",
    "spec_validator = SpecValidator(spec)\n",
    "spec_validator.add_value(MultipleAccuracy.load(\"accuracy across gardens.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur2x8.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur5x8.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur0x8.value\"))\n",
    "spec_validator.add_value(MultipleRanksums.load(\"multiple ranksums for clade2.value\"))\n",
    "spec_validator.add_value(MultipleRanksums.load(\"multiple ranksums between clade2 and 3.value\"))\n",
    "spec_validator.add_value(Integer.load(\"model size.value\"))\n",
    "spec_validator.add_value(CPUStatistics.load(\"predicting cpu.value\"))\n",
    "spec_validator.add_value(MemoryStatistics.load(\"predicting memory.value\"))\n",
    "spec_validator.add_value(Image.load(\"image attributions.value\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate requirements and get validated details.\n",
    "validated_spec = spec_validator.validate()\n",
    "validated_spec.save(force=True)\n",
    "\n",
    "# We want to see the validation results in the Notebook, regardles sof them being saved.\n",
    "validated_spec.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see some of the results of the validation.\n",
    "\n",
    "For example, there is a significant difference between original model with no blur and blur 0x8. So we see a drop in model accuracy with increasing blur. But aside from max blur (0x8), the model accuracy fall off isn't bad.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Report\n",
    "\n",
    "The final step of SDMT involves the generation of a report to communicate the results of model evaluation.\n",
    "\n",
    "TODO: this code needs to be updated to work with the new report format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mlte.report.artifact import Report\n",
    "\n",
    "def unix_timestamp() -> str:\n",
    "    return f\"{int(time.time())}\"\n",
    "\n",
    "def build_report() -> Report:\n",
    "    report = Report()\n",
    "    report.metadata.project_name = \"OxfordFlowerProject\"\n",
    "    report.metadata.authors = [\"Rachel Brower-Sinning\"]\n",
    "    report.metadata.source_url = \"https://github.com/mlte-team\"\n",
    "    report.metadata.artifact_url = \"https://github.com/mlte-team\"\n",
    "    report.metadata.timestamp = unix_timestamp()\n",
    "\n",
    "    report.model_details.name = \"OxfordFlower\"\n",
    "    report.model_details.overview = \"A model that distinguishes among types of flowers.\"\n",
    "    report.model_details.documentation = \"This is a simple identify the category of a flower based on its known categories.\"\n",
    "\n",
    "    report.model_specification.domain = \"Classification\"\n",
    "    report.model_specification.architecture = \"Decision Tree\"\n",
    "    report.model_specification.input = \"Vector[4]\"\n",
    "    report.model_specification.output = \"Binary\"\n",
    "    report.model_specification.data = [\n",
    "        Dataset(\"Dataset0\", \"https://github.com/mlte-team\", \"This is one training dataset.\"),\n",
    "        Dataset(\"Dataset1\", \"https://github.com/mlte-team\", \"This is the other one we used.\"),\n",
    "    ]\n",
    "\n",
    "    report.considerations.users = [\n",
    "        User(\"Botanist\", \"A professional botanist.\"),\n",
    "        User(\"Explorer\", \"A weekend-warrior outdoor explorer.\"),\n",
    "    ]\n",
    "    report.considerations.use_cases = [\n",
    "        UseCase(\"Personal Edification\", \"Quench your curiosity: what species of flower IS that? Wonder no longer.\")\n",
    "    ]\n",
    "    report.considerations.limitations = [\n",
    "        Limitation(\n",
    "            \"Low Training Data Volume\",\n",
    "            \"\"\"\n",
    "            This model was trained on a low volume of training data.\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    "    return report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
