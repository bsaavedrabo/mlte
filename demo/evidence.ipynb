{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect Evidence\n",
    "\n",
    "In the second phase of SDMT, we collect _evidence_ to attest to the fact that the model realized the properties specified in the previous phase.\n",
    "\n",
    "We define and instantiate `Measurement`s to generate this evidence. Each individual piece of evidence is a `Result`. Once `Result`s are produced, we can persist them to an _artifact store_ to maintain our evidence across sessions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries for loading the package locally\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def package_root() -> str:\n",
    "    \"\"\"Resolve the path to the project root.\"\"\"\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src/\"))\n",
    "\n",
    "sys.path.append(package_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which datasets are stored\n",
    "DATASETS_DIR = Path(os.getcwd()) / \"data\"\n",
    "# The path at which models are stored\n",
    "MODELS_DIR = Path(os.getcwd()) / \"models\"\n",
    "# The path at which media is stored\n",
    "MEDIA_DIR = Path(os.getcwd()) / \"media\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "def load_data() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Load machine learning dataset.\n",
    "    :return (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    return train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Train a classifier and save.\"\"\"\n",
    "    X_train, _, y_train, _ = load_data()\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    with (MODELS_DIR / \"model.pkl\").open(\"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training dataset for use by training procedure\n",
    "X_train, _, y_train, _ = load_data()\n",
    "X_train.to_csv(DATASETS_DIR / \"data.csv\")\n",
    "y_train.to_csv(DATASETS_DIR / \"target.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize MLTE Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlte\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "\n",
    "mlte.set_model(\"IrisClassifier\", \"0.0.1\")\n",
    "mlte.set_artifact_store_uri(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Cost Measurements\n",
    "\n",
    "This section demonstrates the simplest possible use-case. We import a MLTE-defined `Measurement`, which is then invoked to produce a `Result`. This result can then be inspected and automatically saved to the artifact store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.result import Integer\n",
    "\n",
    "# Create a measurement\n",
    "measurement = LocalObjectSize(\"model size\")\n",
    "# Execute the measurement\n",
    "size: Integer = measurement.evaluate(MODELS_DIR / \"model.pkl\")\n",
    "\n",
    "# Inspec results\n",
    "print(size)\n",
    "\n",
    "# Save to artifact store\n",
    "size.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Cost Measurements\n",
    "\n",
    "Evidence in this section is largely the same as that demonstrated in the previous section, except it requires some additional setup from the user's perspective. Again, we utilize MLTE-defined `Measurement`s to produce `Result`s that can then be saved to the artifact store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def interpreter_path() -> Path:\n",
    "    \"\"\"Get the path to the current interpreter.\"\"\"\n",
    "    return Path(sys.executable)\n",
    "\n",
    "def script_path() -> Path:\n",
    "    \"\"\"Get the path to the training script.\"\"\"\n",
    "    return (Path(os.getcwd()) / \"train.py\").absolute()\n",
    "\n",
    "def spawn_training_job() -> int:\n",
    "    \"\"\"Spawn the training job and return its process identifier.\"\"\"\n",
    "    python = interpreter_path()\n",
    "    command = [\n",
    "        str(python),\n",
    "        str(script_path()),\n",
    "        \"--dataset-dir\", str(DATASETS_DIR.absolute()),\n",
    "        \"--models-dir\", str(MODELS_DIR.absolute())\n",
    "    ]\n",
    "    p = subprocess.Popen(command)\n",
    "    thread = threading.Thread(target=lambda: p.wait())\n",
    "    thread.start()\n",
    "    return p.pid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first evidence we collect are CPU utilization statistics for a local training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics\n",
    "\n",
    "# Create a measurement\n",
    "measurement = LocalProcessCPUUtilization(\"training cpu\")\n",
    "# Execute the measurement\n",
    "cpu_stats: CPUStatistics = measurement.evaluate(spawn_training_job())\n",
    "\n",
    "# Inspect results\n",
    "print(cpu_stats)\n",
    "\n",
    "# Save to artifact store\n",
    "cpu_stats.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform a similar procedure to measure the memory consumption of a local training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.memory import LocalProcessMemoryConsumption, MemoryStatistics\n",
    "\n",
    "# Create a measurement\n",
    "measurement = LocalProcessMemoryConsumption(\"training memory\")\n",
    "# Execute the measurement\n",
    "mem_stats: MemoryStatistics = measurement.evaluate(spawn_training_job())\n",
    "\n",
    "# Inspect results\n",
    "print(mem_stats)\n",
    "\n",
    "# Save to artifact store\n",
    "mem_stats.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Efficacy Measurements\n",
    "\n",
    "Evidence collected in this section demonstrates MLTE's flexibility in handling inputs from external libraries and in different media types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import tree\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    path = MODELS_DIR / \"model.pkl\"\n",
    "    with path.open(\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "_, X_test, _, y_test = load_data()\n",
    "\n",
    "# Load the model\n",
    "model = load_model()\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test.to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, we simply wrap the output from `accuracy_score` with a builtin MLTE type (`Real`) to integrate it with our growing collection of evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from mlte.measurement.result import Real\n",
    "from mlte.measurement import MeasurementMetadata, Identifier\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = Real(\n",
    "    MeasurementMetadata(\"accuracy_score\", Identifier(\"accuracy\")),\n",
    "    accuracy_score(y_test, y_pred)\n",
    ")\n",
    "\n",
    "# Inspect result\n",
    "print(accuracy)\n",
    "\n",
    "# Save to artifact store\n",
    "accuracy.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next example, we define a custom `Result` type to cope with the output of a third-party library that is not supported by a MLTE builtin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mlte.measurement.result import Result\n",
    "from mlte.measurement import MeasurementMetadata\n",
    "from mlte.measurement.validation import (\n",
    "    Validator,\n",
    "    ValidationResult,\n",
    "    Success,\n",
    "    Failure\n",
    ")\n",
    "\n",
    "class ConfusionMatrix(Result):\n",
    "    def __init__(self, measurement_metadata: MeasurementMetadata, matrix: np.ndarray):\n",
    "        super().__init__(self, measurement_metadata)\n",
    "        \n",
    "        self.matrix: np.ndarray = matrix\n",
    "        \"\"\"Underlying matrix represented as numpy array.\"\"\"\n",
    "\n",
    "    def serialize(self) -> Dict[str, Any]:\n",
    "        return {\"matrix\": [[int(val) for val in row] for row in self.matrix]}\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize(\n",
    "        measurement_metadata: MeasurementMetadata, json_: Dict[str, Any]\n",
    "    ) -> ConfusionMatrix:\n",
    "        return ConfusionMatrix(\n",
    "            measurement_metadata,\n",
    "            np.asarray(json[\"matrix\"])\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.matrix)\n",
    "\n",
    "    def misclassification_count_less_than(self, threshold: int) -> ValidationResult:\n",
    "        return Validator(\n",
    "            \"MisclassCountLessThan\",\n",
    "            lambda cm: Success(\n",
    "                f\"Misclass count {cm.misclassifications} less than threshold {threshold}\"\n",
    "            )\n",
    "            if cm.misclassifications <= threshold\n",
    "            else Failure(\n",
    "                f\"Misclassification count {cm.misclassifications} exceeds threshold {threshold}\"\n",
    "            ),\n",
    "        )(self)\n",
    "\n",
    "    @property\n",
    "    def misclassifications(self) -> int:\n",
    "        count = 0\n",
    "        for i in range(len(self.matrix)):\n",
    "            row = self.matrix[i]\n",
    "            for j in range(len(row)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                count += row[j]\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from mlte.measurement import MeasurementMetadata, Identifier\n",
    "\n",
    "# Generate result\n",
    "matrix = ConfusionMatrix(\n",
    "    MeasurementMetadata(\"confusion_matrix\", Identifier(\"confusion matrix\")),\n",
    "    confusion_matrix(y_test, y_pred)\n",
    ")\n",
    "\n",
    "# Inspect\n",
    "print(matrix)\n",
    "\n",
    "# Save to artifact store\n",
    "matrix.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final example, we demonstrate the ability to integrate other forms of media in our evidence collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlte.measurement.result import Image\n",
    "\n",
    "x = [\"Setosa\", \"Versicolour\", \"Virginica\"]\n",
    "y = [sum(1 for value in y_pred if value == target) for target in [0, 1, 2]]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.title(\"Distribution of Predicted Classes\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.xticks([0, 1, 2])\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.savefig(MEDIA_DIR / \"classes.png\")\n",
    "\n",
    "img = Image(\n",
    "    MeasurementMetadata(\"plt.bar\", Identifier(\"class distribution\")),\n",
    "    MEDIA_DIR / \"classes.png\"\n",
    ")\n",
    "img.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
