# MLTE Mechanics (Redux of *Using MLTE*)

This section provides insight into the mechanics required to use MLTE, whether or not you're following the MLTE process.

The `MLTE` Team believes that effective evaluation starts at the inception of a project, so `MLTE` starts with a negotiation - a discussion about requirements - amongst stakeholders, software engineers, data scientists, and anyone else involved in the project. More information about this can be found in the [Using MLTE](using_mlte.md) Guide.


## Model Testing

MLTE has two types of model testing, internal model testing (IMT) and system dependent model testing (SDMT). The mechanics are similar for both, and this section goes through an example that will allow you to work through either type of testing. Evaluation in `MLTE` follows this process:

- Initialize the `MLTE` context.
- Define a specification.
- Collect evidence.
- Validate results.
- Examine findings.

The scenario used in this section is a hypothetical where visitors to a botanical garden are identifying flowers in the gardens to learn more about them. They are helped by an ML system which uses a model that was trained on the flower category dataset ([Nilsback 2008](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/)).

### Initialize the `MLTE` Context

`MLTE` contains a global context that manages the currently active session. Initializing the context tells MLTE how to store all of the artifacts that it produces.

```python
import os
from mlte.session import set_context, set_store

store_path = os.path.join(os.getcwd(), "store")
os.makedirs(
    store_path, exist_ok=True
)  # Ensure we are creating the folder if it is not there.

set_context("ns", "OxfordFlower", "0.0.1")
set_store(f"local://{store_path}")
```

### Run the `MLTE` UI

Once the context is initialized, you can view all your artifacts in the front end. To run the user interface (UI), run the following in your command line:

```bash
$ mlte ui
```

In order for the frontend to be able to communicate with the store you will need to allow the frontend as an origin.
This can be done by specifying the `--allowed-origins` flag when running the store. 
When ran through the mlte package, the frontend will be hosted at `http://localhost:8000` so the store command will look like this:

```bash
$ mlte store --backend-uri fs://store --allowed-origins http://localhost:8000
```

Once you run it, go to the hosted address to view the `MLTE` UI homepage. 

### Define a Specification

In MLTE, we define requirements by constructing a specification (`Spec`). A `Spec` represents the requirements the completed model must meet in order to be acceptable for use in the system into which it will be integrated. This includes the measurement as well as the threshold for that measurement (a Condition).

#### Conditions

For this flower scenario, our `Spec` includes the following properties and conditions:

- Fairness - Model Impartial to Photo Location
    - The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.
- Robustness- Model Robust to Noise (Image Blur)
    - The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry. The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images. Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.
- Robustness - Model Robust to Noise (Channel Loss)
    - The model receives a picture taken at a garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). The model should still be able to successfully identify the flower at the same rate as full images. Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets. Images with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.
- Performance on Operational Platform
    - The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.
- Interpretability - Understanding Model Results
    - The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.

#### Code for a `Spec`

```python
from mlte.spec.spec import Spec

# The Properties we want to validate, associated with our scenarios.
from mlte.property.costs.storage_cost import StorageCost
from properties.fairness import Fairness
from properties.robustness import Robustness
from properties.interpretability import Interpretability
from properties.predicting_memory_cost import PredictingMemoryCost
from properties.predicting_compute_cost import PredictingComputeCost

# The Value types we will use to validate each condition.
from mlte.measurement.storage import LocalObjectSize
from mlte.measurement.cpu import LocalProcessCPUUtilization
from mlte.measurement.memory import LocalProcessMemoryConsumption
from mlte.value.types.image import Image
from values.multiple_accuracy import MultipleAccuracy
from values.ranksums import RankSums
from values.multiple_ranksums import MultipleRanksums

# The full spec. Note that the Robustness Property contains conditions for both Robustness scenarios.
spec = Spec(
    properties={
        Fairness(
            "Important check if model performs well accross different populations"
        ): {
            "accuracy across gardens": MultipleAccuracy.all_accuracies_more_or_equal_than(
                0.9
            )
        },
        Robustness("Robust against blur and noise"): {
            "ranksums blur2x8": RankSums.p_value_greater_or_equal_to(0.05 / 3),
            "ranksums blur5x8": RankSums.p_value_greater_or_equal_to(0.05 / 3),
            "ranksums blur0x8": RankSums.p_value_greater_or_equal_to(0.05 / 3),
            "multiple ranksums for clade2": MultipleRanksums.all_p_values_greater_or_equal_than(
                0.05
            ),
            "multiple ranksums between clade2 and 3": MultipleRanksums.all_p_values_greater_or_equal_than(
                0.05
            ),
        },
        StorageCost("Critical since model will be in an embedded device"): {
            "model size": LocalObjectSize.value().less_than(3000)
        },
        PredictingMemoryCost(
            "Useful to evaluate resources needed when predicting"
        ): {
            "predicting memory": LocalProcessMemoryConsumption.value().average_consumption_less_than(
                512000.0
            )
        },
        PredictingComputeCost(
            "Useful to evaluate resources needed when predicting"
        ): {
            "predicting cpu": LocalProcessCPUUtilization.value().max_utilization_less_than(
                30.0
            )
        },
        Interpretability("Important to understand what the model is doing"): {
            "image attributions": Image.ignore("Inspect the image.")
        },
    }
)
spec.save(parents=True, force=True)
```

### Collect Evidence

After defining a specification, we can collect evidence to attest to the fact that the model realized the specified properties.

We define and instantiate `Measurements` to generate this evidence. Each individual piece of evidence is a `Value`. Once `Value`s are produced, we can persist them to the artifact store to maintain our evidence.

```python
from pathlib import Path

# The path at which datasets are stored
DATASETS_DIR = Path.cwd() / "data"

# Path where the model files are stored.
MODELS_DIR = Path.cwd() / "model"

# The path at which media is stored
MEDIA_DIR = Path.cwd() / "media"
os.makedirs(MEDIA_DIR, exist_ok=True)
```

Download the model that will be used for some of these measurements.
 
```bash
!sh get_model.sh
```

```python
# General functions.

import garden
import numpy as np


def load_data(data_folder: str):
    """Loads all garden data results and taxonomy categories."""
    df_results = garden.load_base_results(data_folder)
    df_results.head()

    # Load the taxonomic data and merge with results.
    df_info = garden.load_taxonomy(data_folder)
    df_results.rename(columns={"label": "Label"}, inplace=True)
    df_all = garden.merge_taxonomy_with_results(df_results, df_info)

    return df_info, df_all


def split_data(df_info, df_all):
    """Splits the data into 3 different populations to evaluate them."""
    df_gardenpop = df_info.copy()
    df_gardenpop["Population1"] = (
        np.around(
            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],
            decimals=3,
        )
        * 1000
    ).astype(int)
    df_gardenpop["Population2"] = (
        np.around(
            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],
            decimals=3,
        )
        * 1000
    ).astype(int)
    df_gardenpop["Population3"] = (
        np.around(
            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],
            decimals=3,
        )
        * 1000
    ).astype(int)
    df_gardenpop

    # build populations from test data set that match the garden compositions
    from random import choices

    # build 3 gardens with populations of 1000.
    pop_names = ["Population1", "Population2", "Population3"]
    gardenpops = np.zeros((3, 1000), int)
    gardenmems = np.zeros((3, 1000), int)

    for j in range(1000):
        for i in range(len(df_gardenpop)):
            my_flower = df_gardenpop.iloc[i]["Common Name"]

            for g in range(3):
                n_choices = df_gardenpop.iloc[i][pop_names[g]]
                my_choices = df_all[df_all["Common Name"] == my_flower][
                    "model correct"
                ].to_list()
                my_selection = choices(my_choices, k=n_choices)

                gardenpops[g][j] += sum(my_selection)
                gardenmems[g][j] += len(my_selection)

    gardenpops

    return gardenpops, gardenmems


def calculate_model_performance_acc(gardenpops, gardenmems):
    """Get accucray of models across the garden populations"""
    gardenacc = np.zeros((3, 1000), float)
    for i in range(1000):
        for g in range(3):
            gardenacc[g][i] = gardenpops[g][i] / gardenmems[g][i]
    gardenacc

    model_performance_acc = []
    for g in range(3):
        avg = round(np.average(gardenacc[g][:]), 3)
        std = round(np.std(gardenacc[g][:]), 3)
        min = round(np.amin(gardenacc[g][:]), 3)
        max = round(np.amax(gardenacc[g][:]), 3)
        model_performance_acc.append(round(avg, 3))

        print("%1d %1.3f %1.3f %1.3f %1.3f" % (g, avg, std, min, max))

    return model_performance_acc
```

```python
# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.
data = load_data(DATASETS_DIR)
split_data = split_data(data[0], data[1])
```

In this first example, we simply wrap the output from `accuracy_score` with a custom Result type to cope with the output of a third-party library that is not supported by a MLTE builtin.

#### Accuracy Measurements

```python
from values.multiple_accuracy import MultipleAccuracy
from mlte.measurement import ExternalMeasurement

# Evaluate accuracy, identifier has to be the same one defined in the Spec.
accuracy_measurement = ExternalMeasurement(
    "accuracy across gardens", MultipleAccuracy, calculate_model_performance_acc
)
accuracy = accuracy_measurement.evaluate(split_data[0], split_data[1])

# Inspect value
print(accuracy)

# Save to artifact store
accuracy.save(force=True)
```

#### Robustness Measurements
```python
# General functions.
import pandas as pd


def calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:
    # Calculate the base model accuracy result per data label
    df_pos = (
        df_results[df_results["model correct"] == True].groupby("label").count()
    )
    df_pos.drop(columns=["prediced_label"], inplace=True)
    df_neg = (
        df_results[df_results["model correct"] == False]
        .groupby("label")
        .count()
    )
    df_neg.drop(columns=["prediced_label"], inplace=True)
    df_neg.rename(columns={"model correct": "model incorrect"}, inplace=True)
    df_res = df_pos.merge(
        df_neg, right_on="label", left_on="label", how="outer"
    )
    df_res.fillna(0, inplace=True)
    df_res["model acc"] = df_res["model correct"] / (
        df_res["model correct"] + df_res["model incorrect"]
    )
    df_res["count"] = df_res["model correct"] + df_res["model incorrect"]
    df_res.drop(columns=["model correct", "model incorrect"], inplace=True)
    df_res.head()

    return df_res


def calculate_accuracy_per_set(
    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame
) -> pd.DataFrame:
    # Calculate the model accuracy per data label for each blurred data set
    base_filename = "FlowerModelv1_TestSetResults"
    ext_filename = ".csv"
    set_filename = ["_blur2x8", "_blur5x8", "_blur0x8", "_noR", "_noG", "_noB"]

    col_root = "model acc"

    for fs in set_filename:
        filename = os.path.join(data_folder, base_filename + fs + ext_filename)
        colname = col_root + fs

        df_temp = pd.read_csv(filename)
        df_temp.drop(columns=["Unnamed: 0"], inplace=True)

        df_pos = (
            df_temp[df_temp["model correct"] == True].groupby("label").count()
        )
        df_pos.drop(columns=["prediced_label"], inplace=True)
        df_neg = (
            df_results[df_results["model correct"] == False]
            .groupby("label")
            .count()
        )
        df_neg.drop(columns=["prediced_label"], inplace=True)
        df_neg.rename(
            columns={"model correct": "model incorrect"}, inplace=True
        )
        df_res2 = df_pos.merge(
            df_neg, right_on="label", left_on="label", how="outer"
        )
        df_res2.fillna(0, inplace=True)

        df_res2[colname] = df_res2["model correct"] / (
            df_res2["model correct"] + df_res2["model incorrect"]
        )
        df_res2.drop(columns=["model correct", "model incorrect"], inplace=True)

        df_res = df_res.merge(
            df_res2, right_on="label", left_on="label", how="outer"
        )

    df_res.head()
    return df_res


def print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):
    model_acc = sum(df_res[key] * df_res["count"]) / sum(df_res["count"])
    print(name, model_acc)
```

```python
# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.
df_results = garden.load_base_results(DATASETS_DIR)
df_res = calculate_base_accuracy(df_results)
df_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)
df_info = garden.load_taxonomy(DATASETS_DIR)
df_all = garden.merge_taxonomy_with_results(df_res, df_info, "label", "Label")

# fill in missing model accuracy data
df_all["model acc_noR"].fillna(0, inplace=True)
df_all["model acc_noG"].fillna(0, inplace=True)
df_all["model acc_noB"].fillna(0, inplace=True)
```

Now do the actual measurements. First simply see the model accuracy across blurs.

```python
# view changes in model accuracy
print_model_accuracy(df_res, "model acc", "base model accuracy")
print_model_accuracy(
    df_res, "model acc_blur2x8", "model accuracy with 2x8 blur"
)
print_model_accuracy(
    df_res, "model acc_blur5x8", "model accuracy with 5x8 blur"
)
print_model_accuracy(
    df_res, "model acc_blur0x8", "model accuracy with 0x8 blur"
```

Measure the ranksums (p-value) for all blur cases, using scipy.stats.ranksums and the ExternalMeasurement wrapper.

```python
import scipy.stats

from values.ranksums import RankSums
from mlte.measurement import ExternalMeasurement

my_blur = ["2x8", "5x8", "0x8"]
for i in range(len(my_blur)):
    # Define measurements.
    ranksum_measurement = ExternalMeasurement(
        f"ranksums blur{my_blur[i]}", RankSums, scipy.stats.ranksums
    )

    # Evaluate.
    ranksum: RankSums = ranksum_measurement.evaluate(
        df_res["model acc"], df_res[f"model acc_blur{my_blur[i]}"]
    )

    # Inspect values
    print(ranksum)

    # Save to artifact store
    ranksum.save(force=True)
```

Now to next part of the question- is this equal across the phylogenic groups?

First we will check the effect of blur for Clade 2.

```python
from typing import List

from values.multiple_ranksums import MultipleRanksums

# use the initial result, blur columns to anaylze effect of blur
df_all["delta_2x8"] = df_all["model acc"] - df_all["model acc_blur2x8"]
df_all["delta_5x8"] = df_all["model acc"] - df_all["model acc_blur5x8"]
df_all["delta_0x8"] = df_all["model acc"] - df_all["model acc_blur0x8"]

pops = df_all["Clade2"].unique().tolist()
blurs = [
    "delta_2x8",
    "delta_5x8",
    "delta_0x8",
]

ranksums: List = []
for i in range(len(blurs)):
    for pop1 in pops:
        for pop2 in pops:
            ranksum_measurement = ExternalMeasurement(
                f"ranksums clade2 {pop1}-{pop2} blur{blurs[i]}",
                RankSums,
                scipy.stats.ranksums,
            )
            ranksum: RankSums = ranksum_measurement.evaluate(
                df_all[df_all["Clade2"] == pop1][blurs[i]],
                df_all[df_all["Clade2"] == pop2][blurs[i]],
            )
            print(f"blur {blurs[i]}: {ranksum}")
            ranksums.append({ranksum.identifier: ranksum.array})

multiple_ranksums_meas = ExternalMeasurement(
    f"multiple ranksums for clade2", MultipleRanksums, lambda x: x
)
multiple_ranksums: MultipleRanksums = multiple_ranksums_meas.evaluate(ranksums)
multiple_ranksums.num_pops = len(pops)
multiple_ranksums.save(force=True)
```

Now we check between clade 2 and clade 3.

```python
df_now = (
    df_all[["Clade2", "Clade 3"]]
    .copy()
    .groupby(["Clade2", "Clade 3"])
    .count()
    .reset_index()
)
ps1 = df_now["Clade2"].to_list()
ps2 = df_now["Clade 3"].to_list()
print(df_now)

ranksums: List = []
for k in range(len(blurs)):
    print("\n", blurs[k])
    for i in range(len(ps1)):
        p1c1 = ps1[i]
        p1c2 = ps2[i]
        for j in range(len(ps1)):
            p2c1 = ps1[j]
            p2c2 = ps2[j]
            if (
                len(
                    df_all[
                        (df_all["Clade2"] == p1c1) & (df_all["Clade 3"] == p2c2)
                    ][blurs[k]]
                )
                > 0
                | len(
                    df_all[
                        (df_all["Clade2"] == p2c1) & (df_all["Clade 3"] == p2c2)
                    ][blurs[k]]
                )
                > 0
            ):
                ranksum_measurement = ExternalMeasurement(
                    f"ranksums {p1c1}-{p2c2} - {p2c1}-{p2c2} blur{blurs[k]}",
                    RankSums,
                    scipy.stats.ranksums,
                )
                ranksum: RankSums = ranksum_measurement.evaluate(
                    df_all[
                        (df_all["Clade2"] == p1c1) & (df_all["Clade 3"] == p2c2)
                    ][blurs[k]],
                    df_all[
                        (df_all["Clade2"] == p2c1) & (df_all["Clade 3"] == p2c2)
                    ][blurs[k]],
                )
                ranksums.append({ranksum.identifier: ranksum.array})

multiple_ranksums_meas = ExternalMeasurement(
    f"multiple ranksums between clade2 and 3", MultipleRanksums, lambda x: x
)
multiple_ranksums: MultipleRanksums = multiple_ranksums_meas.evaluate(ranksums)
multiple_ranksums.num_pops = len(ps1)
multiple_ranksums.save(force=True)
```

#### Performance Measurements

Now we collect stored, CPU and memory usage data when predicting with the model, for the Performance scenario. NOTE: the version of tensorflow used in this demo requires running it under Python 3.9 or higher.

```python
# This is the external script that will load and run the model for inference/prediction.
script = Path.cwd() / "model_predict.py"
args = [
    "--images",
    DATASETS_DIR,
    "--model",
    MODELS_DIR / "model_f3_a.json",
    "--weights",
    MODELS_DIR / "model_f_a.h5",
]
```

```python
from mlte.measurement.storage import LocalObjectSize
from mlte.value.types.integer import Integer

store_measurement = LocalObjectSize("model size")
size: Integer = store_measurement.evaluate(MODELS_DIR)
print(size)
size.save(force=True)
```

```python
from mlte.measurement import ProcessMeasurement
from mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics

cpu_measurement = LocalProcessCPUUtilization("predicting cpu")
cpu_stats: CPUStatistics = cpu_measurement.evaluate(
    ProcessMeasurement.start_script(script, args)
)
print(cpu_stats)
cpu_stats.save(force=True)
```

```python
from mlte.measurement.memory import (
    LocalProcessMemoryConsumption,
    MemoryStatistics,
)

mem_measurement = LocalProcessMemoryConsumption("predicting memory")
mem_stats: MemoryStatistics = mem_measurement.evaluate(
    ProcessMeasurement.start_script(script, args)
)
print(mem_stats)
mem_stats.save(force=True)
```

#### Interpretability Measurements

Now we proceed to gather data about the Interpretability of the model, for the corresponding scenario. NOTE: the version of tensorflow used in this demo requires running it under Python 3.9 or higher.

```python
model_filename = (
    MODELS_DIR / "model_f3_a.json"
)  # The json file of the model to load
weights_filename = MODELS_DIR / "model_f_a.h5"  # The weights file for the model
```

```python
from model_analysis import *

# Load the model/
loaded_model = load_model(model_filename, weights_filename)
# Load and show the image.

flower_img = "flower3.jpg"  # Filename of flower image to use, public domain image adapted from: https://commons.wikimedia.org/wiki/File:Beautiful_white_flower_in_garden.jpg
flower_idx = (
    42  # Classifier index of associated flower (see OxfordFlower102Labels.csv)
)

im = read_image(os.path.join(DATASETS_DIR, flower_img))

plt.imshow(im)
plt.axis("off")
plt.show()
```

```python
predictions = run_model(im, loaded_model)

baseline, alphas = generate_baseline_and_alphas()
interpolated_images = interpolate_images(
    baseline=baseline, image=im, alphas=alphas
)
```

```python
fig = plt.figure(figsize=(20, 20))

i = 0
for alpha, image in zip(alphas[0::10], interpolated_images[0::10]):
    i += 1
    plt.subplot(1, len(alphas[0::10]), i)
    plt.title(f"alpha: {alpha:.1f}")
    plt.imshow(image)
    plt.axis("off")

plt.tight_layout()
```

```python
path_gradients = compute_gradients(
    loaded_model=loaded_model,
    images=interpolated_images,
    target_class_idx=flower_idx,
)
print(path_gradients.shape)
```

```python
ig = integral_approximation(gradients=path_gradients)
print(ig.shape)
ig_attributions = integrated_gradients(
    baseline=baseline,
    image=im,
    target_class_idx=flower_idx,
    loaded_model=loaded_model,
    m_steps=240,
)
print(ig_attributions.shape)
```

```python
fig = plot_img_attributions(
    image=im,
    baseline=baseline,
    target_class_idx=flower_idx,
    loaded_model=loaded_model,
    m_steps=240,
    cmap=plt.cm.inferno,
    overlay_alpha=0.4,
)

plt.savefig(MEDIA_DIR / "attributions.png")
```

```python
from mlte.measurement import ExternalMeasurement
from mlte.value.types.image import Image

# Save to MLTE store.
img_collector = ExternalMeasurement("image attributions", Image)
img = img_collector.ingest(MEDIA_DIR / "attributions.png")
img.save(force=True)
```

### Validate Results

The final phase of SDMT involves aggregating evidence, validating the metrics reflected by the evidence we collected, and displaying this information in a report.

```python
import os
from mlte.session import set_context, set_store
from pathlib import Path

store_path = os.path.join(os.getcwd(), "store")
os.makedirs(
    store_path, exist_ok=True
)  # Ensure we are creating the folder if it is not there.

set_context("ns", "OxfordFlower", "0.0.1")
set_store(f"local://{store_path}")

# The path at which reports are stored
REPORTS_DIR = Path(os.getcwd()) / "reports"
os.makedirs(REPORTS_DIR, exist_ok=True)
```

Validate Values and get an updated ValidatedSpec with Results

Now that we have our Spec ready and we have enough evidence, we create a SpecValidator with our spec, and add all the Values we have. With that we can validate our spec and generate an output ValidatedSpec, with the validation results.

```python
from mlte.spec.spec import Spec
from mlte.validation.spec_validator import SpecValidator
from mlte.value.artifact import Value

# Load the specification
spec = Spec.load()

# Add all values to the validator.
spec_validator = SpecValidator(spec)
spec_validator.add_values(Value.load_all())
```

```python
# Validate requirements and get validated details.
validated_spec = spec_validator.validate()
validated_spec.save(force=True)

# We want to see the validation results in the Notebook, regardles sof them being saved.
validated_spec.print_results()
```

Here we see some of the results of the validation.

For example, there is a significant difference between original model with no blur and blur 0x8. So we see a drop in model accuracy with increasing blur. But aside from max blur (0x8), the model accuracy fall off isn't bad.

### Generate a Report

Generate a report to communicate the results of model evaluation.

```python
from mlte.model.shared import (
    ProblemType,
    GoalDescriptor,
    MetricDescriptor,
    ModelProductionDescriptor,
    ModelInterfaceDescriptor,
    ModelInputDescriptor,
    ModelOutputDescriptor,
    ModelResourcesDescriptor,
    RiskDescriptor,
    DataDescriptor,
    DataClassification,
    FieldDescriptor,
    LabelDescriptor,
)
from mlte.report.artifact import (
    Report,
    SummaryDescriptor,
    PerformanceDesciptor,
    IntendedUseDescriptor,
    CommentDescriptor,
    QuantitiveAnalysisDescriptor,
)

report = Report(
    summary=SummaryDescriptor(
        problem_type=ProblemType.CLASSIFICATION, task="Flower classification"
    ),
    performance=PerformanceDesciptor(
        goals=[
            GoalDescriptor(
                description="The model should perform well.",
                metrics=[
                    MetricDescriptor(
                        description="accuracy",
                        baseline="Better than random chance.",
                    )
                ],
            )
        ]
    ),
    intended_use=IntendedUseDescriptor(
        usage_context="A handheld flower identification device.",
        production_requirements=ModelProductionDescriptor(
            integration="integration",
            interface=ModelInterfaceDescriptor(
                input=ModelInputDescriptor(description="Vector[150]"),
                output=ModelOutputDescriptor(description="Vector[3]"),
            ),
            resources=ModelResourcesDescriptor(
                cpu="1", gpu="0", memory="6MiB", storage="2KiB"
            ),
        ),
    ),
    risks=RiskDescriptor(
        fp="The wrong type of flower is identified.",
        fn="The flower is not identified.",
        other="N/A",
    ),
    data=[
        DataDescriptor(
            description="Flower dataset.",
            classification=DataClassification.UNCLASSIFIED,
            access="None",
            fields=[
                FieldDescriptor(
                    name="Sepal length",
                    description="The length of the sepal.",
                    type="float",
                    expected_values="N/A",
                    missing_values="N/A",
                    special_values="N/A",
                )
            ],
            labels=[
                LabelDescriptor(description="Dahlia", percentage=30.0),
                LabelDescriptor(description="Sunflower", percentage=30.0),
                LabelDescriptor(description="Azalea", percentage=40.0),
            ],
            policies="N/A",
            rights="N/A",
            source="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
            identifiable_information="N/A",
        )
    ],
    comments=[
        CommentDescriptor(
            content="This model should not be used for nefarious purposes."
        )
    ],
    quantitative_analysis=QuantitiveAnalysisDescriptor(
        content="Insert graph here."
    ),
    validated_spec_id=validated_spec.identifier,
)

report.save(force=True, parents=True)
```